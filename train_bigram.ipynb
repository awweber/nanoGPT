{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdaf706",
   "metadata": {},
   "source": [
    "# Projekt: Ein \"Bigram\"-Langugage Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b606a",
   "metadata": {},
   "source": [
    "Import of required libraries for building a bigram language model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a8488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa7b7f",
   "metadata": {},
   "source": [
    "Hyperparameter-Definitionen für das Bigram-Sprachmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9897c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter ---\n",
    "batch_size = 32 # How many independent sequences to process in parallel\n",
    "block_size = 8  # Maximum length of context (irrelevant for Bigram, but important for later)\n",
    "max_iters = 3000 # number of training iterations\n",
    "eval_interval = 300 # how often to evaluate the loss\n",
    "learning_rate = 1e-2 # learning rate for optimizer\n",
    "eval_iters = 200 # number of iterations for loss estimation\n",
    "\n",
    "# device configuration\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu' # M4 Check!\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda0b07",
   "metadata": {},
   "source": [
    "## 1. Load data and tokenization\n",
    "\n",
    "Loading data from a text file and creating character-level tokenization\n",
    "\n",
    "**Tokenization & Encoding**\n",
    "Wir nutzen hier Character-Level Tokenization. a -> 1, b -> 2.\n",
    "\n",
    "Modernere Modelle wie GPT-4 nutzen \"Sub-word Tokenization\" (Tiktoken), wo häufige Wortteile (z.B. \"ing\" oder \"Pre\") ein einziges Token sind. Für unser Verständnis reicht Character-Level völlig aus und macht den Code schlanker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881ed94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/tinyshakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2dd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o {DATAPATH} https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "364fabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data loaded.\n",
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load text data\n",
    "with open(DATAPATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(\"Text data loaded.\")\n",
    "    print(f\"Length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fddf8f",
   "metadata": {},
   "source": [
    "Sorting and Mapping of characters to indices and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b34c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Sorting and Mapping of characters to indices and vice versa\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All unique characters:\", ''.join(chars))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Mapping: Zeichen zu Integers (Tokenization)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # string to int\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # int to string\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: String -> Liste von ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: Liste von ints -> String\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d77a59",
   "metadata": {},
   "source": [
    "Data preparation: splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63f41db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Convert the entire text into a list of token IDs\n",
    "# Split into training and validation data\n",
    "n = int(0.9*len(data)) # 90% for Training, 10% for Validation\n",
    "train_data = data[:n] # train_data \n",
    "val_data = data[n:] # val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a62ee",
   "metadata": {},
   "source": [
    "Auxillary functions for data batching and loss estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "234c9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function: Data batching ---\n",
    "def get_batch(split):\n",
    "    # Generates a small batch of inputs (x) and targets (y)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # We choose random starting points in the text\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x is the context, y is the target (the next character)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # Move to the M4\n",
    "    return x, y\n",
    "\n",
    "# --- Helper function: Loss estimation (without backprop) ---\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval() # set model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f0189",
   "metadata": {},
   "source": [
    "## 2. Bigram Language Model Definition\n",
    "\n",
    "### What is a Bigram Model?\n",
    "\n",
    "A bigram model is a simple statistical language model that predicts the probability of a word based on the one word that precedes it. It works by analyzing pairs of consecutive words (bigrams) from a training corpus to learn which words tend to follow others, a technique based on the Markov assumption. This allows it to generate new text that mimics the patterns of the original text, though it doesn't understand meaning or consider more than the previous word.  \n",
    "\n",
    "### How it works?\n",
    "\n",
    "1. **Data Training**: The model is trained on a large body of text (a corpus). \n",
    "2. **Counting Bigrams**: It counts how many times each pair of consecutive words appears in the corpus. \n",
    "3. **Probability Calculation**: It calculates the probability of a word appearing given the previous word. For example, the probability of \"cat\" following \"the\" is the count of \"the cat\" divided by the count of \"the\". \n",
    "4. **Text Generation**: When generating new text, it uses these probabilities to predict the next word. For instance, after generating the word \"the,\" it will look at the learned probabilities to decide which word is most likely to come next. \n",
    "5. **Simplification**: It operates under the assumption that a word's probability only depends on the immediately preceding word, ignoring any words that came before that. \n",
    "\n",
    "### Key characteristics\n",
    "\n",
    "1. **Simple yet powerful**: It is a fundamental and effective way to build a basic language model without complex neural networks. \n",
    "2. **Word dependency**: It is better than models that only consider individual words (unigrams) because it captures some local word dependencies. \n",
    "3. **Limited context**: Its main limitation is its narrow view, as it only considers one preceding word and has no memory of further context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a964e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        # Embedding Dimension = Vocab Size, since we have no Hidden Layers\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshape for loss computation\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, _ = self(idx)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B,C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
    "        return idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a84ce8",
   "metadata": {},
   "source": [
    "**Note to Embeddings (nn.Embedding Layer == Table):**\n",
    "\n",
    "In this simple model, the embedding table does not yet function as a semantic vector space (like \"King - Man + Woman = Queen\"). Here it is a simple lookup table. When the model sees the letter \"a\", it looks up row \"a\" in the table. There are probability scores (logits) for all possible letters that could come next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55daae7",
   "metadata": {},
   "source": [
    "## 3. Initialization and Training of Bigram Language Model\n",
    "\n",
    "### Model initialization\n",
    "\n",
    "Initialize the model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf5e9656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and move to device\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device) # Move model to M4\n",
    "\n",
    "# Optimizer (AdamW is standard for LLMs)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8bc0c",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Training the bigram language model using mini-batch gradient descent and periodic loss estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfa6984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "Step 0: Train Loss 4.7627, Val Loss 4.7633\n",
      "Step 300: Train Loss 2.8415, Val Loss 2.8635\n",
      "Step 600: Train Loss 2.5515, Val Loss 2.5881\n",
      "Step 900: Train Loss 2.4970, Val Loss 2.5305\n",
      "Step 1200: Train Loss 2.4908, Val Loss 2.5049\n",
      "Step 1500: Train Loss 2.4716, Val Loss 2.5028\n",
      "Step 1800: Train Loss 2.4557, Val Loss 2.4971\n",
      "Step 2100: Train Loss 2.4725, Val Loss 2.4942\n",
      "Step 2400: Train Loss 2.4637, Val Loss 2.4956\n",
      "Step 2700: Train Loss 2.4644, Val Loss 2.4894\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training ...\")\n",
    "for iter in range(max_iters):\n",
    "    # Every eval_interval iterations, estimate loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f597e8a",
   "metadata": {},
   "source": [
    "## 4. Save the trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15849bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modell-Gewichte gespeichert unter: models/bigram_shakespeare.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/bigram_shakespeare.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nModell-Gewichte gespeichert unter: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d2beb",
   "metadata": {},
   "source": [
    "## 5. Deployment and Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a239b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of text:\n",
      "\n",
      "LLELATI k coput bainuthas I' chie athotorde us m wh.\n",
      "QUCO:\n",
      "Fit hye my n wanofeaver nd blkerd FReps to mas or,\n",
      "\n",
      "\n",
      "US:\n",
      "Hof?\n",
      "Manoorer h mene be e llpueangbavyoy, frmact te Nonthaixt frel amanl;\n",
      "Thin CIUSH:\n",
      "\n",
      "KENCI t me t wh,\n",
      "ARS:\n",
      "LAliavere t,\n",
      "ING herer.\n",
      "\n",
      "JUSThevin pine lir h ss:\n",
      "ABethe, s wce misso tayo sourlimede agn ant f whatithis monorcupr t wis io theas yow, bes, the atssen\n",
      "Thaeadead by, he, whe re,\n",
      "arend d ha LAnt s CIAnofave, r ough e ss bu thais!\n",
      "\n",
      "De unime itsery;\n",
      "\n",
      "ouncl t wise be VO, wimame \n"
     ]
    }
   ],
   "source": [
    "print(\"Generation of text:\")\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # start with a single zero token\n",
    "generated_indices = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
