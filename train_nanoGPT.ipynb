{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdaf706",
   "metadata": {},
   "source": [
    "# Project: Transformer-based Language Model from Scratch\n",
    "\n",
    "Training of a nano-GPT style Transformer language model on tiny Shakespeare dataset.\n",
    "\n",
    "Conceptual Overview\n",
    "\n",
    "The heart of modern LLMs is the Self-Attention mechanism.\n",
    "\n",
    "**Concept: Why Attention?**\n",
    "\n",
    "In the Bigram model, when processing the sentence \"The dog barks,\" the model at the word \"barks\" no longer knew that \"dog\" came before. A Transformer looks at all previous tokens and dynamically decides which ones are important.\n",
    "\n",
    "This works through three vectors that each token possesses (the so-called \"Key, Query, Value\" analogy):\n",
    "\n",
    "1. Query (Q): What am I looking for? (e.g., \"I am a verb, I'm looking for the subject that performs the action\").\n",
    "2. Key (K): What do I offer? (e.g., \"I am a noun/subject\").\n",
    "3. Value (V): What is my actual content? (e.g., \"dog\").\n",
    "\n",
    "When Query and Key match (high mathematical similarity), then much of the Value flows into the current token. Here is the mathematical formula that we will see in the code shortly:\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b606a",
   "metadata": {},
   "source": [
    "### Import of required libraries for building a bigram language model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88a8488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa7b7f",
   "metadata": {},
   "source": [
    "Hyperparameter-Definitionen für das Bigram-Sprachmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9897c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter settings for training a transformer model\n",
    "batch_size = 32      # number of sequences processed in parallel\n",
    "max_iters = 5000 # Reduced for quicker training\n",
    "eval_interval = 250     # evaluate every 250 steps\n",
    "learning_rate = 3e-4 # slightly lower for more complex networks\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "# device configuration\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu' # M4 Check!\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda0b07",
   "metadata": {},
   "source": [
    "## 1. Load data and tokenization\n",
    "\n",
    "Loading data from a text file and creating character-level tokenization\n",
    "\n",
    "**Tokenization & Encoding**\n",
    "Wir nutzen hier Character-Level Tokenization. a -> 1, b -> 2.\n",
    "\n",
    "Modernere Modelle wie GPT-4 nutzen \"Sub-word Tokenization\" (Tiktoken), wo häufige Wortteile (z.B. \"ing\" oder \"Pre\") ein einziges Token sind. Für unser Verständnis reicht Character-Level völlig aus und macht den Code schlanker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b9572af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/tinyshakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d0a6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o {DATAPATH} https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "364fabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data loaded.\n",
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load text data\n",
    "with open(DATAPATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(\"Text data loaded.\")\n",
    "    print(f\"Length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fddf8f",
   "metadata": {},
   "source": [
    "Sorting and Mapping of characters to indices and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6b34c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Sorting and Mapping of characters to indices and vice versa\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All unique characters:\", ''.join(chars))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Mapping: Zeichen zu Integers (Tokenization)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # string to int\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # int to string\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: String -> Liste von ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: Liste von ints -> String\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d77a59",
   "metadata": {},
   "source": [
    "Data preparation: splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63f41db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Convert the entire text into a list of token IDs\n",
    "# Split into training and validation data\n",
    "n = int(0.9*len(data)) # 90% for Training, 10% for Validation\n",
    "train_data = data[:n] # train_data \n",
    "val_data = data[n:] # val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f0189",
   "metadata": {},
   "source": [
    "## 2. Definition of Transformer Model Components\n",
    "\n",
    "### What is a Transformer Model?\n",
    "\n",
    "A transformer model is a type of neural network architecture that is designed to process sequential data, such as text. Unlike traditional recurrent neural networks (RNNs), which process data sequentially, transformers use a mechanism called self-attention to weigh the importance of different words in a sentence, regardless of their position. This allows transformers to capture long-range dependencies and relationships between words more effectively.\n",
    "\n",
    "### How it works?\n",
    "\n",
    "1. **Tokens & Positional Encoding**: The model is trained on a large body of text (a corpus). Each word is converted into a vector (embedding), and positional encodings are added to give the model information about the position of each word in the sequence.\n",
    "2. **Self-Attention Mechanism**: The core innovation. It lets the model weigh how relevant every other word in the input is to the current word, understanding context (e.g., what \"it\" refers to in a sentence). It uses self-attention to compute a representation of each word in the context of all other words in the sentence.\n",
    "3. **Multi-Head Attention**: Instead of having a single attention mechanism, transformers use multiple attention heads to capture different types of relationships and dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a84ce8",
   "metadata": {},
   "source": [
    "**Note to Embeddings (nn.Embedding Layer == Table):**\n",
    "\n",
    "In this simple model, the embedding table does not yet function as a semantic vector space (like \"King - Man + Woman = Queen\"). Here it is a simple lookup table. When the model sees the letter \"a\", it looks up row \"a\" in the table. There are probability scores (logits) for all possible letters that could come next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55daae7",
   "metadata": {},
   "source": [
    "## 3. Initialization and Training of Transformer Model\n",
    "\n",
    "### Model initialization\n",
    "\n",
    "Initialize the model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf5e9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 10788929\n"
     ]
    }
   ],
   "source": [
    "from llmlib import nanoGPT\n",
    "\n",
    "# Hyperparameter\n",
    "n_embd = 384     # size of the embedding vectors (dimension)\n",
    "n_head = 6       # number of attention heads (384 / 6 = 64 dim per head)\n",
    "n_layer = 6      # number of transformer blocks\n",
    "block_size = 256 # Context: The model looks back 256 characters\n",
    "vocab_size = 65  # number of unique characters in the vocabulary\n",
    "dropout = 0.2    # against overfitting\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# initialize the model and move to device\n",
    "model = nanoGPT.GPTLanguageModel()\n",
    "model = model.to(device) # Move model to M4\n",
    "# Print number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimizer (AdamW is standard for LLMs)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dd4ba",
   "metadata": {},
   "source": [
    "Auxiliary functions for data batching and loss estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa118a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function: Data batching ---\n",
    "def get_batch(split):\n",
    "    # Generates a small batch of inputs (x) and targets (y)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # We choose random starting points in the text\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x is the context, y is the target (the next character)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # Move to the M4\n",
    "    return x, y\n",
    "\n",
    "# --- Helper function: Loss estimation (without backprop) ---\n",
    "@torch.no_grad() # Disable gradient tracking for efficiency\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval() # set model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8bc0c",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Training the bigram language model using mini-batch gradient descent and periodic loss estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dfa6984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ... (this may take a while)\n",
      "Step 0: Train Loss 4.3329, Val Loss 4.3363\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 250: Train Loss 2.4187, Val Loss 2.4492\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 500: Train Loss 2.1459, Val Loss 2.1931\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 750: Train Loss 1.8992, Val Loss 2.0110\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 1000: Train Loss 1.7299, Val Loss 1.8751\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 1250: Train Loss 1.6188, Val Loss 1.7879\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 1500: Train Loss 1.5468, Val Loss 1.7358\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 1750: Train Loss 1.4889, Val Loss 1.6901\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 2000: Train Loss 1.4439, Val Loss 1.6416\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 2250: Train Loss 1.4030, Val Loss 1.6090\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 2500: Train Loss 1.3719, Val Loss 1.5933\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 2750: Train Loss 1.3464, Val Loss 1.5644\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 3000: Train Loss 1.3191, Val Loss 1.5557\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 3250: Train Loss 1.2994, Val Loss 1.5503\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 3500: Train Loss 1.2775, Val Loss 1.5252\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 3750: Train Loss 1.2591, Val Loss 1.5194\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 4000: Train Loss 1.2427, Val Loss 1.5128\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 4250: Train Loss 1.2269, Val Loss 1.5090\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 4500: Train Loss 1.2172, Val Loss 1.5047\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 4750: Train Loss 1.2039, Val Loss 1.5024\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n",
      "Step 4999: Train Loss 1.1838, Val Loss 1.5020\n",
      "Model saved to train/nanoGPT_shakespeare.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to save the trained model\n",
    "train_path = 'train/nanoGPT_shakespeare.pt'\n",
    "best_train_loss = float('inf')\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Start training ... (this may take a while)\")\n",
    "for iter in range(max_iters):\n",
    "    # Every eval_interval iterations, estimate loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
    "        # Save the model if the validation loss is the best we've seen so far\n",
    "        if losses['val'] < best_val_loss and losses['train'] < best_train_loss:\n",
    "            best_val_loss = float(losses['val'])\n",
    "            best_train_loss = float(losses['train'])\n",
    "            torch.save(model.state_dict(), train_path)\n",
    "            print(f\"Model saved to {train_path}\")\n",
    "\n",
    "\n",
    "    # Get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c2296",
   "metadata": {},
   "source": [
    "## 4. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6f4e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modell-Gewichte gespeichert unter: models/nano_gpt_shakespeare.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/nano_gpt_shakespeare.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nModell-Gewichte gespeichert unter: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d2beb",
   "metadata": {},
   "source": [
    "## 5. Deployment and Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a239b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of text:\n",
      "\n",
      "Title there.\n",
      "\n",
      "THOMASTASASTINGBROKE:\n",
      "Moonio!\n",
      "Ay, not in her mistrikes to be power a bone.\n",
      "Is it is't, a man: belihful, and ask to the bright;\n",
      "My garments are not a woman-placed clied\n",
      "And Itally kneets and respere deaught.\n",
      "\n",
      "GLOUCESTER:\n",
      "Here's humother string for your doop!\n",
      "That the valour it eyes of Edward Raven and a hrone?\n",
      "\n",
      "EDWARD:\n",
      "Ay, let's flesh the confesseth of dale was train'd\n",
      "'What thou shalt slay that I say\n",
      "Bear I do the rather and my fair trow'd,\n",
      "And learn that I mean, all my favour,\n",
      "Whi\n"
     ]
    }
   ],
   "source": [
    "print(\"Generation of text:\")\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # start with a single zero token\n",
    "generated_indices = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
