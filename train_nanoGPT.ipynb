{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcdaf706",
   "metadata": {},
   "source": [
    "# Project: Transformer-based Language Model from Scratch\n",
    "\n",
    "Training of a nano-GPT style Transformer language model on tiny Shakespeare dataset.\n",
    "\n",
    "Conceptual Overview\n",
    "\n",
    "The heart of modern LLMs is the Self-Attention mechanism.\n",
    "\n",
    "**Concept: Why Attention?**\n",
    "\n",
    "In the Bigram model, when processing the sentence \"The dog barks,\" the model at the word \"barks\" no longer knew that \"dog\" came before. A Transformer looks at all previous tokens and dynamically decides which ones are important.\n",
    "\n",
    "This works through three vectors that each token possesses (the so-called \"Key, Query, Value\" analogy):\n",
    "\n",
    "1. Query (Q): What am I looking for? (e.g., \"I am a verb, I'm looking for the subject that performs the action\").\n",
    "2. Key (K): What do I offer? (e.g., \"I am a noun/subject\").\n",
    "3. Value (V): What is my actual content? (e.g., \"dog\").\n",
    "\n",
    "When Query and Key match (high mathematical similarity), then much of the Value flows into the current token. Here is the mathematical formula that we will see in the code shortly:\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b606a",
   "metadata": {},
   "source": [
    "### Import of required libraries for building a bigram language model using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a8488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa7b7f",
   "metadata": {},
   "source": [
    "Hyperparameter-Definitionen für das Bigram-Sprachmodell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9897c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter ---\n",
    "batch_size = 64 # More power through M4\n",
    "block_size = 256 # Context: The model looks back 256 characters\n",
    "max_iters = 5000 # Reduced for quicker training\n",
    "eval_interval = 250     # evaluate every 250 steps\n",
    "learning_rate = 3e-4 # slightly lower for more complex networks\n",
    "eval_iters = 200\n",
    "n_embd = 384     # size of the embedding vectors (dimension)\n",
    "n_head = 6       # number of attention heads (384 / 6 = 64 dim per head)\n",
    "n_layer = 6      # number of transformer blocks\n",
    "dropout = 0.2    # against overfitting\n",
    "\n",
    "# device configuration\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu' # M4 Check!\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda0b07",
   "metadata": {},
   "source": [
    "## 1. Load data and tokenization\n",
    "\n",
    "Loading data from a text file and creating character-level tokenization\n",
    "\n",
    "**Tokenization & Encoding**\n",
    "Wir nutzen hier Character-Level Tokenization. a -> 1, b -> 2.\n",
    "\n",
    "Modernere Modelle wie GPT-4 nutzen \"Sub-word Tokenization\" (Tiktoken), wo häufige Wortteile (z.B. \"ing\" oder \"Pre\") ein einziges Token sind. Für unser Verständnis reicht Character-Level völlig aus und macht den Code schlanker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9572af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/tinyshakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0a6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o {DATAPATH} https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364fabbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data loaded.\n",
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load text data\n",
    "with open(DATAPATH, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(\"Text data loaded.\")\n",
    "    print(f\"Length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fddf8f",
   "metadata": {},
   "source": [
    "Sorting and Mapping of characters to indices and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b34c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Sorting and Mapping of characters to indices and vice versa\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"All unique characters:\", ''.join(chars))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Mapping: Zeichen zu Integers (Tokenization)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # string to int\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # int to string\n",
    "encode = lambda s: [stoi[c] for c in s] # Encoder: String -> Liste von ints\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decoder: Liste von ints -> String\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d77a59",
   "metadata": {},
   "source": [
    "Data preparation: splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63f41db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Convert the entire text into a list of token IDs\n",
    "# Split into training and validation data\n",
    "n = int(0.9*len(data)) # 90% for Training, 10% for Validation\n",
    "train_data = data[:n] # train_data \n",
    "val_data = data[n:] # val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a62ee",
   "metadata": {},
   "source": [
    "Auxillary functions for data batching and loss estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234c9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function: Data batching ---\n",
    "def get_batch(split):\n",
    "    # Generates a small batch of inputs (x) and targets (y)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # We choose random starting points in the text\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # x is the context, y is the target (the next character)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # Move to the M4\n",
    "    return x, y\n",
    "\n",
    "# --- Helper function: Loss estimation (without backprop) ---\n",
    "@torch.no_grad() # Disable gradient tracking for efficiency\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval() # set model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69f0189",
   "metadata": {},
   "source": [
    "## 2. Definition of Transformer Model Components\n",
    "\n",
    "### What is a Transformer Model?\n",
    "\n",
    "A transformer model is a type of neural network architecture that is designed to process sequential data, such as text. Unlike traditional recurrent neural networks (RNNs), which process data sequentially, transformers use a mechanism called self-attention to weigh the importance of different words in a sentence, regardless of their position. This allows transformers to capture long-range dependencies and relationships between words more effectively.\n",
    "\n",
    "### How it works?\n",
    "\n",
    "1. **Tokens & Positional Encoding**: The model is trained on a large body of text (a corpus). Each word is converted into a vector (embedding), and positional encodings are added to give the model information about the position of each word in the sequence.\n",
    "2. **Self-Attention Mechanism**: The core innovation. It lets the model weigh how relevant every other word in the input is to the current word, understanding context (e.g., what \"it\" refers to in a sentence). It uses self-attention to compute a representation of each word in the context of all other words in the sentence.\n",
    "3. **Multi-Head Attention**: Instead of having a single attention mechanism, transformers use multiple attention heads to capture different types of relationships and dependencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a964e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One attention head \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # Register buffer, so that it is not a trainable parameter\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Lower triangular matrix\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # type: ignore # Masking future tokens\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,head_size)\n",
    "        out = wei @ v # (B,T,head_size)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple Heads in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Output der Heads konkatenieren\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" Ein einfaches lineares Layer gefolgt von Nicht-Linearität \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), # Expansion (Standard in Transformern)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection zurück\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block: Kommunikation gefolgt von Berechnung \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) # Communication\n",
    "        self.ffwd = FeedFoward(n_embd)                  # Computation\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual Connections (x + ...) sind extrem wichtig für tiefes Lernen!\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661dae4a",
   "metadata": {},
   "source": [
    "### The GPT Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f126cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \"\"\" The GPT Language Model \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding für Token-Identität\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Embedding für Position im Satz (WICHTIG!)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        # Die Transformer Blöcke\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # Finaler Layer Norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # Projektion auf Vokabular\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token Emb + Positional Emb\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        \n",
    "        # Durch die Transformer Blöcke\n",
    "        x = self.blocks(x) \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Kontext beschneiden, wenn er zu lang wird (wir haben max block_size PosEmbeddings)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a84ce8",
   "metadata": {},
   "source": [
    "**Note to Embeddings (nn.Embedding Layer == Table):**\n",
    "\n",
    "In this simple model, the embedding table does not yet function as a semantic vector space (like \"King - Man + Woman = Queen\"). Here it is a simple lookup table. When the model sees the letter \"a\", it looks up row \"a\" in the table. There are probability scores (logits) for all possible letters that could come next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55daae7",
   "metadata": {},
   "source": [
    "## 3. Initialization and Training of Transformer Model\n",
    "\n",
    "### Model initialization\n",
    "\n",
    "Initialize the model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf5e9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 10788929\n"
     ]
    }
   ],
   "source": [
    "# initialize the model and move to device\n",
    "model = GPTLanguageModel()\n",
    "model = model.to(device) # Move model to M4\n",
    "# Print number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimizer (AdamW is standard for LLMs)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8bc0c",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "Training the bigram language model using mini-batch gradient descent and periodic loss estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfa6984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ... (this may take a while)\n",
      "Step 0: Train Loss 4.3327, Val Loss 4.3355\n",
      "Step 250: Train Loss 2.3843, Val Loss 2.4145\n",
      "Step 500: Train Loss 2.0241, Val Loss 2.0974\n",
      "Step 750: Train Loss 1.7672, Val Loss 1.9069\n",
      "Step 1000: Train Loss 1.6152, Val Loss 1.7761\n",
      "Step 1250: Train Loss 1.5155, Val Loss 1.7087\n",
      "Step 1500: Train Loss 1.4436, Val Loss 1.6387\n",
      "Step 1750: Train Loss 1.3956, Val Loss 1.6084\n",
      "Step 2000: Train Loss 1.3482, Val Loss 1.5897\n",
      "Step 2250: Train Loss 1.3177, Val Loss 1.5654\n",
      "Step 2500: Train Loss 1.2822, Val Loss 1.5357\n",
      "Step 2750: Train Loss 1.2556, Val Loss 1.5129\n",
      "Step 3000: Train Loss 1.2312, Val Loss 1.5123\n",
      "Step 3250: Train Loss 1.2116, Val Loss 1.5103\n",
      "Step 3500: Train Loss 1.1880, Val Loss 1.4995\n",
      "Step 3750: Train Loss 1.1678, Val Loss 1.4918\n",
      "Step 4000: Train Loss 1.1518, Val Loss 1.4965\n",
      "Step 4250: Train Loss 1.1316, Val Loss 1.5024\n",
      "Step 4500: Train Loss 1.1117, Val Loss 1.4885\n",
      "Step 4750: Train Loss 1.0938, Val Loss 1.4892\n",
      "Step 4999: Train Loss 1.0801, Val Loss 1.5056\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training ... (this may take a while)\")\n",
    "for iter in range(max_iters):\n",
    "    # Every eval_interval iterations, estimate loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c2296",
   "metadata": {},
   "source": [
    "## 4. Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f4e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modell-Gewichte gespeichert unter: models/nano_gpt_shakespeare.pt\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/nano_gpt_shakespeare.pt\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nModell-Gewichte gespeichert unter: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54d2beb",
   "metadata": {},
   "source": [
    "## 5. Deployment and Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a239b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of text:\n",
      "\n",
      "Little heaven, and finst thee, believe thee\n",
      "And not in him plainted stones.\n",
      "\n",
      "Provost:\n",
      "What more, than my bosom, here had they were they?\n",
      "And withal: this woman a two, Tramn.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Say thou art a just: donestrate hour reposed:\n",
      "Most redeat frtning fools, thou spoke\n",
      "Look'd with outragely madam?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I am gled time well.\n",
      "\n",
      "LUCIO:\n",
      "By good, sir, to-morrow.\n",
      "\n",
      "Nurse:\n",
      "Should you hence!\n",
      "\n",
      "ESpator:\n",
      "When you are now toe, and since mewhat worse't, what's\n",
      "stain'd off cales.\n",
      "Bad, there, Vau\n"
     ]
    }
   ],
   "source": [
    "print(\"Generation of text:\")\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device) # start with a single zero token\n",
    "generated_indices = model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "print(decode(generated_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
